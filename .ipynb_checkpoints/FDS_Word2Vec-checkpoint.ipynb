{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ausgewählte Kapitel sozial Webtechnologien - Neuronale Netze\n",
    "## Trainieren eines Word2Vec Modells und Darstellung von Wort- und Dokumentenvektoren anhand von Anfragetexten des FragDenStaat-Projektes\n",
    "\n",
    "Bearbeiten von:\n",
    "* Sebastian Jüngling (558556)\n",
    "* Konstantin Bruckert (558290)\n",
    "\n",
    "Prüfer:\n",
    "* Benjamin Voigt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einleitung\n",
    "Über das FragDenStaat-Portal werden Anfragen an Behörden in Deutschland gesammelt und zur Verfügung gestellt. Durch die stetig wachsende Popularität des Portals liegt diesem Projekt ein umfangreicher Datensatz vor, dessen Informationsgehalt im Laufe dieses Notebooks mit Hilfe eines Word2Vec Modells möglichst weit ausgeschöpft werden soll.\n",
    "\n",
    "Grober Ablauf: Zunächst werden die bereits bereinigten Daten für die Weiterverarbeitung aufbereitet und randomisiert. Tatsächliche Input-Daten werden daraufhin durch die indexierung und Speicherung in Lookup-Tables generiert. Mithilfe von Context-Windows können nun Input-Target-Wörter mit entsprechenden Labels aus Daten abgeleitet und für alle Sätze erzeugt werden. Das eigentlich Kernstück des Notebooks bildet dann das Skip-Gramm Modell als Hidden-Layer, welches in sequentiellen Batches über mehrere Epochen die Weights per Loss trainiert. Die daraus resultierenden Word-Embeddings können nun genutzt werden um Dokumenten-Vektoren aufzubauen. Diese oder auch einfache Wort-Vektoren können mithilfe der Cosine-Similarity verglichen werden. Abschließend wird versucht, die Word-Embeddings in verschieden Arten und unter zuhilfenahme des TSNE-Algorithmus zu visualiseren bzw. Ähnlichkeiten zu clustern.\n",
    "\n",
    "Für allgemeine hintergrundinformationen zu den einzelnen hier im Modul angewandte Techniken und auch Details zur Entscheidungsfindung lohnt sich zudem ein Blick in das [Exposé](Documents/NN-Projekt-Expose.pdf).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "us2jJuCTNzou"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from tempfile import gettempdir\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import xrange\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from plotly.graph_objs import *\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten:\n",
    "Hier passiert das Einladen des Anfragen-Katalogs des FragDenStaat-Projektes.  \n",
    "Die Daten wurden schon im Zuge der Werkstudententätigkeit von S. Jüngling im Vorfeld bezogen und weitestgehend aufbereitet.\n",
    "Dabei wurden die Texte auf ihre bedeutungstragenden Begriffe reduziert und die Wörter lemmatisiert.\n",
    "\n",
    "Bitte achten Sie darauf, die Daten gemäß der Anleitung in der README.md Datei herunterzuladen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "colab_type": "code",
    "id": "6jy-dki-Sbxc",
    "outputId": "19e9db64-9618-4bad-bf42-fee7bb83b4dd"
   },
   "outputs": [],
   "source": [
    "data = pd.read_json('fds_requests_preprocessed.json', orient='records', encoding='utf-8')\n",
    "data = data.set_index('id') #set column 'id' as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "colab_type": "code",
    "id": "3VhuI9WkXbeH",
    "outputId": "5128ec4d-42eb-4cc7-fbab-7ed22836cf73",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>preprocessed</th>\n",
       "      <th>textrank</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47033</th>\n",
       "      <td>1. Wann haben die beiden letzten lebensmittelr...</td>\n",
       "      <td>[[kontrollbericht, parkstern, berlin], [betrie...</td>\n",
       "      <td>[[parkstern, Parkstern, 1.1227777778], [berlin...</td>\n",
       "      <td>Kontrollbericht zu Parkstern, Berlin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131943</th>\n",
       "      <td>Die Stellungnahme des BfR zur IARC- Monographi...</td>\n",
       "      <td>[[stellungnahme, bfr], [iarc, monographie, gly...</td>\n",
       "      <td>[[stellungnahme, Stellungnahme, 1.0], [bfr, Bf...</td>\n",
       "      <td>Stellungnahme des BfR zur IARC- Monographie üb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47827</th>\n",
       "      <td>1. Wann haben die beiden letzten lebensmittelr...</td>\n",
       "      <td>[[kontrollbericht, aroma, berlin], [betriebsüb...</td>\n",
       "      <td>[[aroma, Aroma, 1.1227777778], [berlin, Berlin...</td>\n",
       "      <td>Kontrollbericht zu Aroma, Berlin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131938</th>\n",
       "      <td>Die Stellungnahme des BfR zur IARC- Monographi...</td>\n",
       "      <td>[[stellungnahme, bfr], [iarc, monographie, gly...</td>\n",
       "      <td>[[stellungnahme, Stellungnahme, 1.0], [bfr, Bf...</td>\n",
       "      <td>Stellungnahme des BfR zur IARC- Monographie üb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48091</th>\n",
       "      <td>1. Wann haben die beiden letzten lebensmittelr...</td>\n",
       "      <td>[[kontrollbericht], [hans, glück, bonn], [betr...</td>\n",
       "      <td>[[bonn, Bonn, 1.2479166667000001], [hans, Hans...</td>\n",
       "      <td>Kontrollbericht zu \"Hans im Glück\", Bonn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              description  \\\n",
       "id                                                          \n",
       "47033   1. Wann haben die beiden letzten lebensmittelr...   \n",
       "131943  Die Stellungnahme des BfR zur IARC- Monographi...   \n",
       "47827   1. Wann haben die beiden letzten lebensmittelr...   \n",
       "131938  Die Stellungnahme des BfR zur IARC- Monographi...   \n",
       "48091   1. Wann haben die beiden letzten lebensmittelr...   \n",
       "\n",
       "                                             preprocessed  \\\n",
       "id                                                          \n",
       "47033   [[kontrollbericht, parkstern, berlin], [betrie...   \n",
       "131943  [[stellungnahme, bfr], [iarc, monographie, gly...   \n",
       "47827   [[kontrollbericht, aroma, berlin], [betriebsüb...   \n",
       "131938  [[stellungnahme, bfr], [iarc, monographie, gly...   \n",
       "48091   [[kontrollbericht], [hans, glück, bonn], [betr...   \n",
       "\n",
       "                                                 textrank  \\\n",
       "id                                                          \n",
       "47033   [[parkstern, Parkstern, 1.1227777778], [berlin...   \n",
       "131943  [[stellungnahme, Stellungnahme, 1.0], [bfr, Bf...   \n",
       "47827   [[aroma, Aroma, 1.1227777778], [berlin, Berlin...   \n",
       "131938  [[stellungnahme, Stellungnahme, 1.0], [bfr, Bf...   \n",
       "48091   [[bonn, Bonn, 1.2479166667000001], [hans, Hans...   \n",
       "\n",
       "                                                    title  \n",
       "id                                                         \n",
       "47033                Kontrollbericht zu Parkstern, Berlin  \n",
       "131943  Stellungnahme des BfR zur IARC- Monographie üb...  \n",
       "47827                    Kontrollbericht zu Aroma, Berlin  \n",
       "131938  Stellungnahme des BfR zur IARC- Monographie üb...  \n",
       "48091            Kontrollbericht zu \"Hans im Glück\", Bonn  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beispiel für Preprocessing des Anfragetextes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titel und Description für Beispielanfrage:\n",
      "\n",
      "Kontrollbericht zu Aroma, Berlin\n",
      "1. Wann haben die beiden letzten lebensmittelrechtlichen Betriebsüberprüfungen im folgenden Betrieb stattgefunden:\r\n",
      "Aroma\r\n",
      "Kantstraße\r\n",
      "10625 Berlin\r\n",
      "\r\n",
      "2. Kam es hierbei zu Beanstandungen? Falls ja, beantrage ich hiermit die Herausgabe des entsprechenden Kontrollberichts an mich.\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "Preprocessed Anfragetext für Titel und Anfragetext:\n",
      "\n",
      "[['kontrollbericht', 'aroma', 'berlin'], ['betriebsüberprüfungen', 'betrieb'], ['aroma', 'kantstraße', 'berlin'], ['beanstandung'], ['herausgabe', 'kontrollberichts']]\n"
     ]
    }
   ],
   "source": [
    "print('Titel und Description für Beispielanfrage:\\n')\n",
    "print(data.loc[47827]['title'])\n",
    "print(data.loc[47827]['description'])\n",
    "\n",
    "print('-------------------------------------------------------------------------')\n",
    "\n",
    "print('\\nPreprocessed Anfragetext für Titel und Anfragetext:\\n')\n",
    "print(data.loc[47827]['preprocessed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduzierung der Anzahl der Anfragen zu Glyphosat\n",
    "Immer wieder kommt es bei FragDenStaat zu einer Häufung tagespolitischer Themen, wie z.B. die Fragen rund um das Thema Glyphosat. Seit März 2019 sind hierzu bereits mehr als 30.000 Anfragen eingegangen, welche Anhand eines immergleichen Musters ausformuliert werden und somit das Training der globalen Datenmenge zu stark beeinflussen. Zur Reduzierung dieses Einflusses werden die Anfragen zu diesem Thema bei 3000 gedeckelt.\n",
    "Um auch in Zukunft und ggf. bei der Nutzung aktuellerer/andersartiger Datensätze einwandfreie Ergebnisse zu erzielen, sollte regelmäßig geprüft werden, ob die Daten von einem bestimmten Thema dominiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der Anfragen zu Glyphosat: 3000 von insgesamt: 59168 Anfragen\n",
      "Anzahl der Anfragen zu Glyphosat nach Bereinigung: 3000 von insgesamt: 59168 Anfragen\n"
     ]
    }
   ],
   "source": [
    "glyphosat_title = 'Stellungnahme des BfR zur IARC- Monographie über Glyphosat'\n",
    "glyphosat_ids = data[data['title'] == glyphosat_title].index\n",
    "\n",
    "print('Anzahl der Anfragen zu Glyphosat:', len(glyphosat_ids), 'von insgesamt:', len(data), 'Anfragen')\n",
    "\n",
    "# Da dies alle gleiche Anfragen sind und diese hohe Anzahl den Trainingsprozess verfälschen würde, \n",
    "# wird die Anzahl der Anfragen zu Glyphosat auf 3000 beschränkt\n",
    "remain_glyphosat_requests = 3000\n",
    "data.drop(glyphosat_ids[remain_glyphosat_requests:], inplace=True) # drop by id\n",
    "\n",
    "print('Anzahl der Anfragen zu Glyphosat nach Bereinigung:', len(data[data['title'] == glyphosat_title]), 'von insgesamt:', len(data), 'Anfragen')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle Data:\n",
    "Während des Trainings mit den Daten konnte trotz der sorgfältigen Beseitigung von dominanten Themen immer noch eine überproportionale Gewichtung der Glyphosat-Themen festgestellt werden. Das Problem lag hierbei im chronologisch vorliegenden Datensatz, welcher eine natürliche, große Abfolge von gleichen Themen nacheinander besitzt. Um die Word-Embeddings dadurch nicht zu stark in eine Richtung zu trainieren, werden die Anfragen randomisiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>preprocessed</th>\n",
       "      <th>textrank</th>\n",
       "      <th>title</th>\n",
       "      <th>doc_vector</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50326</th>\n",
       "      <td>1. Wann haben die beiden letzten lebensmittelr...</td>\n",
       "      <td>[[manh, hoa, do, |, asia-imbiss, mögeldorf, nü...</td>\n",
       "      <td>[[do, Do, 1.1317027778], [|, |, 1.1317027778],...</td>\n",
       "      <td>Manh Hoa Do | Asia-Imbiss Mögeldorf, Nürnberg</td>\n",
       "      <td>[0.63906544, 0.37667438, 0.2606645, 0.15448664...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23730</th>\n",
       "      <td>Durch Kontakt mit einer  Fachabteilung wurde m...</td>\n",
       "      <td>[[herausgabe, weisung], [arbeitsanweisungen, k...</td>\n",
       "      <td>[[mitarbeiter, Mitarbeitern, 2.9120482067], [a...</td>\n",
       "      <td>Herausgabe der internen Weisungen / Arbeitsanw...</td>\n",
       "      <td>[0.5262659, 0.24017203, -0.017450962, -0.06750...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2432</th>\n",
       "      <td>Ich bitte um Zusendung des folgenden Gutachten...</td>\n",
       "      <td>[[frage, ausgestaltung, stabilitätsmechanismus...</td>\n",
       "      <td>[[frage, Fragen, 1.0], [ausgestaltung, Ausgest...</td>\n",
       "      <td>Finanzverfassungsrechtliche Fragen zur Ausgest...</td>\n",
       "      <td>[0.35916921, 0.23056042, 0.08289202, 0.1130204...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13203</th>\n",
       "      <td>die Ausarbeitung WD 10 - 074/08 mit dem Titel ...</td>\n",
       "      <td>[[rahmenbedingung, verhältnis, staat, religion...</td>\n",
       "      <td>[[rahmenbedingung, Rahmenbedingungen, 1.306141...</td>\n",
       "      <td>WD 10 - 074/08 – Rechtliche Rahmenbedingungen ...</td>\n",
       "      <td>[0.27358878, 0.25355667, -0.1087888, -0.187085...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48378</th>\n",
       "      <td>1. Wann haben die beiden letzten lebensmittelr...</td>\n",
       "      <td>[[kontrollbericht, metzgerei, albrecht, huglfi...</td>\n",
       "      <td>[[metzgerei, Metzgerei, 1.0814583333], [albrec...</td>\n",
       "      <td>Kontrollbericht zu Metzgerei Albrecht, Huglfing</td>\n",
       "      <td>[0.60347605, 0.3083321, 0.028805416, 0.0739764...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             description  \\\n",
       "id                                                         \n",
       "50326  1. Wann haben die beiden letzten lebensmittelr...   \n",
       "23730  Durch Kontakt mit einer  Fachabteilung wurde m...   \n",
       "2432   Ich bitte um Zusendung des folgenden Gutachten...   \n",
       "13203  die Ausarbeitung WD 10 - 074/08 mit dem Titel ...   \n",
       "48378  1. Wann haben die beiden letzten lebensmittelr...   \n",
       "\n",
       "                                            preprocessed  \\\n",
       "id                                                         \n",
       "50326  [[manh, hoa, do, |, asia-imbiss, mögeldorf, nü...   \n",
       "23730  [[herausgabe, weisung], [arbeitsanweisungen, k...   \n",
       "2432   [[frage, ausgestaltung, stabilitätsmechanismus...   \n",
       "13203  [[rahmenbedingung, verhältnis, staat, religion...   \n",
       "48378  [[kontrollbericht, metzgerei, albrecht, huglfi...   \n",
       "\n",
       "                                                textrank  \\\n",
       "id                                                         \n",
       "50326  [[do, Do, 1.1317027778], [|, |, 1.1317027778],...   \n",
       "23730  [[mitarbeiter, Mitarbeitern, 2.9120482067], [a...   \n",
       "2432   [[frage, Fragen, 1.0], [ausgestaltung, Ausgest...   \n",
       "13203  [[rahmenbedingung, Rahmenbedingungen, 1.306141...   \n",
       "48378  [[metzgerei, Metzgerei, 1.0814583333], [albrec...   \n",
       "\n",
       "                                                   title  \\\n",
       "id                                                         \n",
       "50326      Manh Hoa Do | Asia-Imbiss Mögeldorf, Nürnberg   \n",
       "23730  Herausgabe der internen Weisungen / Arbeitsanw...   \n",
       "2432   Finanzverfassungsrechtliche Fragen zur Ausgest...   \n",
       "13203  WD 10 - 074/08 – Rechtliche Rahmenbedingungen ...   \n",
       "48378    Kontrollbericht zu Metzgerei Albrecht, Huglfing   \n",
       "\n",
       "                                              doc_vector  \n",
       "id                                                        \n",
       "50326  [0.63906544, 0.37667438, 0.2606645, 0.15448664...  \n",
       "23730  [0.5262659, 0.24017203, -0.017450962, -0.06750...  \n",
       "2432   [0.35916921, 0.23056042, 0.08289202, 0.1130204...  \n",
       "13203  [0.27358878, 0.25355667, -0.1087888, -0.187085...  \n",
       "48378  [0.60347605, 0.3083321, 0.028805416, 0.0739764...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.sample(frac=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input-Daten generieren:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten aufbereiten\n",
    "Für die späteren Bearbeitungsschritte müssen die vorliegenden Anfragen aufbereitet und um Metainformationen erweitert werden.  \n",
    "Insbesondere die Indezierung der benutzten Wörter und das erstellen entsprechender Lookup-Tables spielt im weiteren Verlauf eine wichtige Rolle. \n",
    "Mit den daraus gewonnenen Wort-Indizes werden die Sätze der Anfragetexte nachgebaut. Für den Trainigsprozess wird außerdem die Gesamtanzahl der einzigartigen Wörter in allen Anfragetexten benötigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5nxCqoQSTYBn"
   },
   "outputs": [],
   "source": [
    "def build_lookup_tables(docs, vocabulary_size=None):\n",
    "    '''\n",
    "    :param docs: Spalte eines pandas-DF: data['preprocessed'].values\n",
    "    :return sentences: Alle Sätze aller Dokumente in einer Liste\n",
    "    :return words: Alle Wörter aller Dokumente in einer Liste\n",
    "    :return word_count: Häufigkeiten der jeweiligen Wörter in allen Dokumenten\n",
    "    :return word_2_index_dict: \n",
    "    :return index_2_word_dict:\n",
    "    :return sentences_as_index: Alle Sätze aller Dokumente mit Wortindex, anstatt des Wortes\n",
    "    :return sentences_as_index_flattened: wie sentences_as_index, aber ohne subarrays\n",
    "    :return vocabulary_size: Anzahl der unique Wörter\n",
    "    '''\n",
    "    sentences = [sent for pd_list in docs for sent in pd_list]\n",
    "    words = [word for sent in sentences for word in sent]\n",
    "  \n",
    "    if not vocabulary_size:\n",
    "        # unique word count\n",
    "        vocabulary_size = len(set(words)) # because of unknown word\n",
    " \n",
    "    # count words\n",
    "    word_count = collections.Counter(words).most_common(vocabulary_size)\n",
    "    word_count.append(['UNK', -1]) # flag for words which are not common enqough\n",
    "  \n",
    "    # lookup-tables\n",
    "    word_2_index_dict = {}\n",
    "    for index, word in enumerate(word_count):\n",
    "        word_2_index_dict[word[0]] = index\n",
    "  \n",
    "    index_2_word_dict = dict(zip(word_2_index_dict.values(), word_2_index_dict.keys()))\n",
    "  \n",
    "    # Wörter der Anfragetexte durch Indizes austauschen:\n",
    "    sentences_as_index = []\n",
    "    unknown_word_count = 0\n",
    "    for sent in sentences:\n",
    "        sent_index = []\n",
    "        for word in sent:\n",
    "            if word in word_2_index_dict:\n",
    "                sent_index.append(word_2_index_dict[word])\n",
    "            else:\n",
    "                unknown_word_count += 1\n",
    "        if sent_index:\n",
    "            sentences_as_index.append(sent_index)\n",
    "    word_count[-1][1] = unknown_word_count\n",
    "\n",
    "    sentences_as_index_flattened = [word for sent in sentences_as_index for word in sent]\n",
    "  \n",
    "    return sentences, words, word_count, word_2_index_dict, index_2_word_dict, sentences_as_index, sentences_as_index_flattened, vocabulary_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2axsWl9ii25H"
   },
   "outputs": [],
   "source": [
    "sentences, words, word_count, word_2_index_dict, index_2_word_dict, sentences_as_index, sentences_as_index_flattened, vocabulary_size = build_lookup_tables(data['preprocessed'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beispiele:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alle Sätze der Anfragetexte in einer Liste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "htJa8Opejkgl",
    "outputId": "592c135e-96e3-4ae4-c706-0b372efaee26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['krankenhaus', 'berlin'],\n",
       " ['möglichkeit',\n",
       "  'datenzugriffs',\n",
       "  'hilfe',\n",
       "  'edv',\n",
       "  '-landeskrankenhäusern',\n",
       "  'b.',\n",
       "  'vivantes',\n",
       "  'friedrichshain',\n",
       "  'vivantes',\n",
       "  'spandau'],\n",
       " ['grundlage', 'zugriffsmöglichkeit'],\n",
       " ['möglichkeit',\n",
       "  'datenzugriffs',\n",
       "  'hilfe',\n",
       "  'edv',\n",
       "  'abteilung',\n",
       "  'charité',\n",
       "  'berlin',\n",
       "  'b.',\n",
       "  'kardiologie',\n",
       "  'dermatologie',\n",
       "  'behandlung',\n",
       "  'patient',\n",
       "  'abteilung',\n",
       "  'jahr'],\n",
       " ['grundlage', 'zugriffsmöglichkeit']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flattened data: only sentences\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alle Wörter aller Anfragetexte chronologisch in einer Liste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CC0x0bS2jk93",
    "outputId": "7658adac-e306-4c36-c37b-73b83cd2fa77"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['krankenhaus', 'berlin', 'möglichkeit', 'datenzugriffs', 'hilfe']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all words in docs\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anzahl der Worthäufigkeiten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "o7AXFIVPjlPc",
    "outputId": "40571fd8-1d1a-4008-c7a3-2bc918fe47dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('betrieb', 24351),\n",
       " ('herausgabe', 24007),\n",
       " ('kontrollbericht', 23946),\n",
       " ('beanstandung', 23880),\n",
       " ('betriebsüberprüfungen', 23782)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count of words\n",
    "word_count[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wort zu Wortindex Lookuptable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "uYVuDGdRjlin",
    "outputId": "a2c897db-6785-478f-8d9f-fcaa4f980c49"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'betrieb': 0,\n",
       " 'herausgabe': 1,\n",
       " 'kontrollbericht': 2,\n",
       " 'beanstandung': 3,\n",
       " 'betriebsüberprüfungen': 4,\n",
       " 'kontrollberichts': 5,\n",
       " 'dokument': 6,\n",
       " 'information': 7,\n",
       " 'abs.': 8,\n",
       " 'stellungnahme': 9}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word_2_index_dict\n",
    "# nur für Anschauungszwecke:\n",
    "{k: word_2_index_dict[k] for k in list(word_2_index_dict)[:10]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wortindex zu Wort Lookuptable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "2ZhDExldjlzf",
    "outputId": "f71b8bb2-bbbd-477c-b86e-5c9b8c4f4316"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'betrieb',\n",
       " 1: 'herausgabe',\n",
       " 2: 'kontrollbericht',\n",
       " 3: 'beanstandung',\n",
       " 4: 'betriebsüberprüfungen',\n",
       " 5: 'kontrollberichts',\n",
       " 6: 'dokument',\n",
       " 7: 'information',\n",
       " 8: 'abs.',\n",
       " 9: 'stellungnahme'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#index_2_word_dict\n",
    "# nur für Anschauungszwecke:\n",
    "{k: index_2_word_dict[k] for k in list(index_2_word_dict)[:10]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alle Wörter in allen Anfragesätzen ausgetauscht durch den jeweiligen Wortindex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "xjRpEGCGkHud",
    "outputId": "023dd201-62d0-4556-b4f9-c903648f7778"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1020, 15],\n",
       " [250, 13598, 779, 6703, 53908, 543, 10897, 5901, 10897, 4419],\n",
       " [225, 13599],\n",
       " [250, 13598, 779, 6703, 358, 2397, 15, 543, 12371, 53909, 783, 588, 358, 10],\n",
       " [225, 13599]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_as_index[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alle Wörter aller Anfragetexte chronologisch in einer Liste, ausgetauscht durch den Wortindex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pnrYmbeJMVpm",
    "outputId": "825ce233-a7f8-4b29-b506-e1b878de3bd7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1020, 15, 250, 13598, 779, 6703, 53908, 543, 10897, 5901]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_as_index_flattened[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anzahl aller einzigartigen Wörter aus allen Anfragetexten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mDjh9g9Tk5WS",
    "outputId": "36a24c11-c75f-4bfc-a283-b5fd71bbf659"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95602"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input-Target-Wörter mit entsprechenden Labels aus Daten ableiten\n",
    "[INFO] Ich hab jetzt den Abschnitt zu Language Modell und Skip-Gram als auch Word-Embeddings hier hin vorgezogen. Hier war ursprünglich nur Context-Window. Aber ich finde es gut, wenn man hier schon den Background mit den Word-Embeds erklärt. Was meinst du?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "Essentiell für die folgenden Schritte ist ein generelles Verständnis von Word Embeddings. Ein Wort kann als ein Vektor mit einer beliebigen Anzahl von Features dargestellt werden. Die voreingestellten Parameter dieses Notebooks arbeiten mit 300 Features.\n",
    "Vergleichen wir beispielsweise die Word Embeddings der Wörter \"Hund\" und \"Katze\", so werden bestimmte Features innerhalb der beiden Vektoren eine Ähnlichkeit haben, u.a. an der Stelle wo das Modell die Kategorie \"Tier\" trainiert hat. Anhand eines Beispiels, in dem die Word Embedding Values mit Farben je nach Wert ersetzt wurden, lässt sich dieses Prinzip gut veranschaulichen:\n",
    "\n",
    "<img src=\"Images/king-man-woman-embedding.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "TODO/QUELLE: https://jalammar.github.io/illustrated-word2vec/\n",
    "\n",
    "\n",
    "### Language Modelling\n",
    "Je nach Ansatz ist es das Ziel eines Language Modells, für ein gegebenes Wort möglichst Präzise vorhersagen zu treffen, welches Wort darauf folgen könnte (CBOW) oder anhand eines Wortes die umgebenden Wörter vorherzusagen (Skipgram). Anhand der im vorherigen Abschnitt erstellten Word Embeddings können wir einzelne Wörter oder ganze Sätze leicht vergleichen und prüfen, ob Sie in einem kontextuellen Zusammenhang stehen. \n",
    "\n",
    "### Context Window (Skip-Gram Modell)\n",
    "Natürlich muss ein solches Language-Modell ausgiebig trainiert werden. Mittels Context-Windows, also ein Ausschnitt von umgebenden Wörtern, veruschen wir Wort-Paare aus Target-Wörtern und Label-Wörtern zu erstellen, die häufig zusammen auftreten. Angenommen wir nutzen Window-Size=2, dann betrachten wir in jedem Wort eines Satzes die zwei Wörter (\"labels\") vor und nach dem fokusierten Wort (\"target\") und notieren dieses gemeinsame Auftreten. In folgendem Beispiel wird die Context-Window Methode an einem Skip-Gram Modell dargestellt. Das Wort \"red\" ist ein target (=output) und die jeweils umgebenden Wörter sind die labels (=input):\n",
    "\n",
    "<img src=\"Images/skipgram-sliding-window-samples.png\" alt=\"drawing\" width=\"400\"/>\n",
    "TODO/QUELLE: https://jalammar.github.io/illustrated-word2vec/\n",
    "\n",
    "Context-Windows arbeiten mit einzelnen Sätzen und beim Beginn und Ende eines jeden Satzes muss zudem darauf geachtet werden, dass mit dem Window nicht vor oder nach dem Satz (Nan) geslided wird. Die folgende Implementierung erzeugt nun die target-label paare, wobei anstelle realer Wörter im weiteren Verlauf die Wort-Indizes genutzt werden (siehe Beispiel). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_-0DEefWVVgF"
   },
   "outputs": [],
   "source": [
    "def get_context_window(input_data, target_index, window_size):\n",
    "    '''\n",
    "    Ermittelt umgebene Wörter abhängig von window_size und target_index des Wortes\n",
    "    :param input_data: Liste mit Wortindizes\n",
    "    :param: target_index: Listenindex des jeweiligen Wortes\n",
    "    :param window_size: Anzahl der Wörter links und rechts des target wortes\n",
    "    :return target value und liste der umgebenen Wörter\n",
    "    '''\n",
    "  \n",
    "    left_start_index = target_index - window_size if (target_index - window_size) >= 0 else 0\n",
    "  \n",
    "    right_start_index = target_index + 1\n",
    "    right_stop_index = target_index + window_size + 1\n",
    "  \n",
    "    target = input_data[target_index]\n",
    "    left_window = input_data[left_start_index:target_index]\n",
    "    right_window = input_data[right_start_index:right_stop_index]\n",
    "  \n",
    "    return target, left_window + right_window\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "tL2BjvxTXoHm",
    "outputId": "8209c344-3151-4a41-dea8-5b03b38096e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beispielsatz aus Indizes: [0, 1, 2, 3, 4, 5]\n",
      "\n",
      "target word index: 0\n",
      "context words für window_size 2: [1, 2]\n",
      "\n",
      "target word index: 2\n",
      "context words für window_size 2: [0, 1, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "#example:\n",
    "input_data = list(range(6))\n",
    "print('Beispielsatz aus Indizes:', input_data)\n",
    "print()\n",
    "\n",
    "target_word_index, context_words = get_context_window(input_data, 0, 2)\n",
    "print('target word index:', target_word_index)\n",
    "print('context words für window_size 2:', context_words)\n",
    "\n",
    "target_word_index, context_words = get_context_window(input_data, 2, 2)\n",
    "print('\\ntarget word index:', target_word_index)\n",
    "print('context words für window_size 2:', context_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generiert Target-Wortliste mit entsprechenden Wortkontexten für alle Sätze der Anfragetexte:\n",
    "Nach den Beispielen kann die Context-Window Funktion nun auf alle Sätze in den Input-Daten angewandt werden. Weitere Beispiele bieten zudem zur besseren veranschaulichung ein Re-Mapping der Word-Indexe auf die realen Wörter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q-3OJiQbbPw1"
   },
   "outputs": [],
   "source": [
    "def build_targets_and_labels(input_data, window_size):\n",
    "    '''\n",
    "    Ermittelt alle targets und labels abhängig von der window_size\n",
    "    :param input_data: Liste mit Sublisten (Sätze)\n",
    "    :param window_size: Anzahl der Wörter links und rechts des target wortes\n",
    "    :return targets: Liste aller Targetwörter\n",
    "    :return labels: Liste aller Labels zu jeweiligem Targetwort\n",
    "    '''\n",
    "    targets = []\n",
    "    labels = []\n",
    "    for sent in input_data:\n",
    "        for index, word in enumerate(sent):\n",
    "            target_word_index, context_words = get_context_window(sent, index, window_size)\n",
    "            for context_word in context_words:\n",
    "                targets.append(target_word_index)\n",
    "                labels.append(context_word)\n",
    "    return targets, labels\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beispiel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "t6nlmt7jgwj3",
    "outputId": "3b343462-481e-42c3-c52b-4dfb6d457db9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erste zwei Beispielsätze mit Wortinidizes:\n",
      " [[1020, 15], [250, 13598, 779, 6703, 53908, 543, 10897, 5901, 10897, 4419]] \n",
      "\n",
      "Target Wortindizes: \n",
      " [1020, 15, 250, 250, 13598, 13598, 13598, 779, 779, 779, 779, 6703, 6703, 6703, 6703, 53908, 53908, 53908, 53908, 543, 543, 543, 543, 10897, 10897, 10897, 10897, 5901, 5901, 5901, 5901, 10897, 10897, 10897, 4419, 4419]\n",
      "Lable Wortindizes: \n",
      " [15, 1020, 13598, 779, 250, 779, 6703, 250, 13598, 6703, 53908, 13598, 779, 53908, 543, 779, 6703, 543, 10897, 6703, 53908, 10897, 5901, 53908, 543, 5901, 10897, 543, 10897, 10897, 4419, 10897, 5901, 4419, 5901, 10897]\n",
      "\n",
      "Beispiel für Mapping der Wortinindizes der Target und Label-liste:\n",
      "1020 krankenhaus -> 15 berlin\n",
      "15 berlin -> 1020 krankenhaus\n",
      "250 möglichkeit -> 13598 datenzugriffs\n",
      "250 möglichkeit -> 779 hilfe\n",
      "13598 datenzugriffs -> 250 möglichkeit\n",
      "13598 datenzugriffs -> 779 hilfe\n",
      "13598 datenzugriffs -> 6703 edv\n",
      "779 hilfe -> 250 möglichkeit\n",
      "779 hilfe -> 13598 datenzugriffs\n",
      "779 hilfe -> 6703 edv\n",
      "779 hilfe -> 53908 -landeskrankenhäusern\n",
      "6703 edv -> 13598 datenzugriffs\n",
      "6703 edv -> 779 hilfe\n",
      "6703 edv -> 53908 -landeskrankenhäusern\n",
      "6703 edv -> 543 b.\n",
      "53908 -landeskrankenhäusern -> 779 hilfe\n",
      "53908 -landeskrankenhäusern -> 6703 edv\n",
      "53908 -landeskrankenhäusern -> 543 b.\n",
      "53908 -landeskrankenhäusern -> 10897 vivantes\n",
      "543 b. -> 6703 edv\n",
      "543 b. -> 53908 -landeskrankenhäusern\n",
      "543 b. -> 10897 vivantes\n",
      "543 b. -> 5901 friedrichshain\n",
      "10897 vivantes -> 53908 -landeskrankenhäusern\n",
      "10897 vivantes -> 543 b.\n",
      "10897 vivantes -> 5901 friedrichshain\n",
      "10897 vivantes -> 10897 vivantes\n",
      "5901 friedrichshain -> 543 b.\n",
      "5901 friedrichshain -> 10897 vivantes\n",
      "5901 friedrichshain -> 10897 vivantes\n",
      "5901 friedrichshain -> 4419 spandau\n",
      "10897 vivantes -> 10897 vivantes\n",
      "10897 vivantes -> 5901 friedrichshain\n",
      "10897 vivantes -> 4419 spandau\n",
      "4419 spandau -> 5901 friedrichshain\n",
      "4419 spandau -> 10897 vivantes\n"
     ]
    }
   ],
   "source": [
    "input_data = sentences_as_index[:2]\n",
    "#input_data = [sentences_as_index_flattened]\n",
    "print('Erste zwei Beispielsätze mit Wortinidizes:\\n', input_data, '\\n')\n",
    "\n",
    "targets_list, labels_list = build_targets_and_labels(input_data, window_size=2)\n",
    "print('Target Wortindizes: \\n', targets_list)\n",
    "print('Lable Wortindizes: \\n', labels_list)\n",
    "\n",
    "print('\\nBeispiel für Mapping der Wortinindizes der Target und Label-liste:')\n",
    "for i in range(len(targets_list)):\n",
    "    print(targets_list[i], index_2_word_dict[targets_list[i]], '->', labels_list[i], index_2_word_dict[labels_list[i]])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Trainings-Batch:\n",
    "Folgende Hilfsfunktion generiert iterativ Teilsequenzen (Batches) aus den target-label Paaren. Im Return-Wert der Funktion ist ein einzelner Batch und mithilfe der Variable `data_index` wird die aktuelle Iterationsposition im Gesamtdatensatz zwischengespeichert und resetted, sobald das Ende des Gesamtdatensatzes erreicht wurde. Für den späteren Trainingsprozess spielt die Batch-Size eine nicht unterhebliche Rolle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2yiUq2UkhBtA"
   },
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "def generate_batch(batch_size, targets_list, labels_list):\n",
    "    '''\n",
    "    Generiert den Trainigs-Batch\n",
    "    #:param window_size: Anzahl der Wörter links und rechts des target wortes [TODO] KANN DAS RAUS???\n",
    "    :return batch: targets\n",
    "    :return labels\n",
    "    '''\n",
    "    global data_index\n",
    "    if data_index + batch_size > len(targets_list):\n",
    "        data_index = 0\n",
    "    batch = np.array(targets_list[data_index:data_index + batch_size], dtype=np.int32)\n",
    "    labels = np.array(labels_list[data_index:data_index + batch_size], dtype=np.int32)[:, np.newaxis]\n",
    "    #labels = np.array(labels_list[data_index:data_index + batch_size], dtype=np.int32).reshape((batch_size,1))\n",
    "    data_index += batch_size\n",
    "  \n",
    "    return batch, labels\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beispiel für batch_size = 8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "iUXnETiMiERm",
    "outputId": "e2299c78-fe77-423f-da90-a9691a4bd611"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Indizes:\n",
      "[ 1020    15   250   250 13598 13598 13598   779]\n",
      "\n",
      "Label Indizes\n",
      "[[   15]\n",
      " [ 1020]\n",
      " [13598]\n",
      " [  779]\n",
      " [  250]\n",
      " [  779]\n",
      " [ 6703]\n",
      " [  250]]\n",
      "\n",
      "Beispiel für Mapping der Wortinindizes der Target und Label-liste:\n",
      "1020 krankenhaus -> 15 berlin\n",
      "15 berlin -> 1020 krankenhaus\n",
      "250 möglichkeit -> 13598 datenzugriffs\n",
      "250 möglichkeit -> 779 hilfe\n",
      "13598 datenzugriffs -> 250 möglichkeit\n",
      "13598 datenzugriffs -> 779 hilfe\n",
      "13598 datenzugriffs -> 6703 edv\n",
      "779 hilfe -> 250 möglichkeit\n"
     ]
    }
   ],
   "source": [
    "batch, labels = generate_batch(8, targets_list, labels_list)\n",
    "print('Target Indizes:')\n",
    "print(batch)\n",
    "\n",
    "print('\\nLabel Indizes')\n",
    "print(labels)\n",
    "\n",
    "print('\\nBeispiel für Mapping der Wortinindizes der Target und Label-liste:')\n",
    "for i in range(len(batch)):\n",
    "    print(batch[i], index_2_word_dict[batch[i]], '->', labels[i, 0], index_2_word_dict[labels[i, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build und Trainiere Skip-Gram Model\n",
    "Bis hierhin sollte klar sein was die Funktion eines Skip-Gram Models und deren target-label Paaren, eines Context Window und eines Batches ist. Mit diesem Wissen steht dem eigentlichen Model-Training nichts mehr im Wege! Fast. Vor dem Training dürfen bietet sich zunächst die letzte Möglichkeit, mit einstellbaren Parametern Einfluss auf den Verlauf des Trainings zu nehmen. Neben den bereits bekannten Begriffen müssen noch ein paar wenige neue Parameter verstanden und eingestellt werden, die einen erheblichen Einfluss auf den weiteren Trainingsverlauf ausüben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einstellbare Parameter:\n",
    "\n",
    "* `batch_size`: Anzahl der berückstichtigten Target-Label-Paare pro Trainingsiteration\n",
    "* `embedding_size`: Dimensionsgröße der Word-Embeddings \n",
    "    * -> Wert von 300 zeigt sich als guter Kompromiss von Performace und Genauigkeit\n",
    "* `skip_window`: Anzahl der berüchtigten Wörter links und rechts (Nachbarwörter) eines Targetwortes für Bildung der Target-Label-Paare\n",
    "* `num_sampled` Anzahl der Negative Samples für NCE Loss (siehe weiter unten)\n",
    "* Anpassung der sich exponentiell verringerten Lernrate (Start-/End-Learning-Rate)\n",
    "* `epochs`: Anzahl der Epochen, also Anzahl der Iterationen die benötigt werden um einmal alle Target-Label-Paare zu durchlaufen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TVj4YN6KTK40"
   },
   "outputs": [],
   "source": [
    "batch_size = 128 \n",
    "embedding_size = 300 # Dimension der Word-Embeddings\n",
    "skip_window = 2  # Anzahl der Wörter links und rechts vom Target-Wort\n",
    "num_sampled = 64 #64 # Anzahl der negative samples\n",
    "\n",
    "# Die Lernrate wird je Iteration verringert:\n",
    "starter_learning_rate = 1.0\n",
    "end_learning_rate = 0.1\n",
    "\n",
    "epochs = 6 # Anzahl der Epochen. Eine Epoche ist eine Iteration durch den kompletten Trainingsbestand\n",
    "print_every_x_step = 2000 # alle x Iterationen werden infos geprintet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generieren der Target- und Labelliste für alle Anfragetexte, basierend auf der eingestellten Skip Window Size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_list, labels_list = build_targets_and_labels(sentences_as_index, window_size=skip_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wichtige fixe Parameter, abgeleitet aus den einstellbaren Parametern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl Target-Label-Paare: 3099230\n",
      "24213 Iterationen werden benötigt um einmal während des Trainings durch alle Target-Label-Paare zu iterieren\n",
      "Iterationen während des Trainings: 145278\n"
     ]
    }
   ],
   "source": [
    "data_index = 0 # für batch start\n",
    "\n",
    "exp_decay_lr = starter_learning_rate - end_learning_rate\n",
    "\n",
    "input_length = len(targets_list) # Anzahl der Target-Label-Paare\n",
    "print('Anzahl Target-Label-Paare:', input_length)\n",
    "\n",
    "full_iteration_cycle = int(math.ceil(input_length / batch_size))\n",
    "print(full_iteration_cycle, 'Iterationen werden benötigt um einmal während des Trainings durch alle Target-Label-Paare zu iterieren')\n",
    "\n",
    "num_steps = full_iteration_cycle * epochs\n",
    "print('Iterationen während des Trainings:', num_steps)\n",
    "\n",
    "epsilon=1e-12 # dont touch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Graph:\n",
    "... Erklärung zur Modellarchitektur, Skipgram, NCE, negative sampling usw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "vpEXenMKUWkk",
    "outputId": "666d61c9-50f4-4931-c5be-800356a3fa4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/konstantin/.virtualenvs/testpy3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/konstantin/.virtualenvs/testpy3/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # Input Daten\n",
    "    with tf.name_scope('inputs'):\n",
    "        train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "        train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "        iteration = tf.placeholder(tf.int32)\n",
    "  \n",
    "    # Benutze CPU:\n",
    "    with tf.device('/cpu:0'):\n",
    "        with tf.name_scope('embeddings'):\n",
    "            #embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "            # init embeddings:\n",
    "            embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], 0.001, 1.0))\n",
    "            # embeddings lookup table:\n",
    "            embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "     \n",
    "        # TODO\n",
    "        # Construct the variables for the NCE loss\n",
    "        with tf.name_scope('weights'):\n",
    "            nce_weights = tf.Variable(\n",
    "                tf.truncated_normal(\n",
    "                    [vocabulary_size, embedding_size],\n",
    "                    stddev=1.0 / math.sqrt(embedding_size)\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Bias:\n",
    "        with tf.name_scope('biases'):\n",
    "            nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # TODO\n",
    "    # Compute the average NCE loss for the batch.\n",
    "    # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "    # time we evaluate the loss.\n",
    "    # Explanation of the meaning of NCE loss:\n",
    "    #   http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(\n",
    "                weights=nce_weights,\n",
    "                biases=nce_biases,\n",
    "                labels=train_labels,\n",
    "                inputs=embed,\n",
    "                num_sampled=num_sampled,\n",
    "                num_classes=vocabulary_size)\n",
    "        )      \n",
    "  \n",
    "    #Lernrate:\n",
    "    with tf.name_scope('lr'):\n",
    "        # Verringert Lernrate exponentiell, abhäging von der aktuellen Trainigsiteration\n",
    "        lr = end_learning_rate +  tf.train.exponential_decay(exp_decay_lr, iteration, 10000, 1/math.e)\n",
    "\n",
    "    # Gradient Descent optimizer mit entsprechender Lernrate und Minimieren des Loss\n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "    # normalize embeddings to be in range [0, 1]\n",
    "    '''normalized_embeddings = tf.math.divide(\n",
    "        tf.subtract(\n",
    "            embeddings,\n",
    "            tf.reduce_min(embeddings)\n",
    "        ),\n",
    "        tf.maximum(\n",
    "            tf.subtract(\n",
    "                tf.reduce_max(embeddings),\n",
    "                tf.reduce_min(embeddings)\n",
    "            ),\n",
    "            epsilon\n",
    "        )\n",
    "    )\n",
    "\n",
    "    #normalized_embeddings = tf.to_float(normalized_embeddings)\n",
    "    normalized_embeddings = tf.dtypes.cast(normalized_embeddings, tf.float32)'''\n",
    " \n",
    "    # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training: \n",
    "... hier passiert das eigtl Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 884
    },
    "colab_type": "code",
    "id": "NB2sTAXdXE5a",
    "outputId": "df06d536-cb21-4ee5-e5c9-2e26b7aeb7eb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration-Step: 0\n",
      "\tAverage loss:\t 302.8840637207031 \n",
      "\tlearning-rate:\t 1.0\n",
      "Iteration-Step: 2000\n",
      "\tAverage loss:\t 146.9957670879364 \n",
      "\tlearning-rate:\t 0.8368577\n",
      "Iteration-Step: 4000\n",
      "\tAverage loss:\t 84.13222938060761 \n",
      "\tlearning-rate:\t 0.7032881\n",
      "Iteration-Step: 6000\n",
      "\tAverage loss:\t 59.96004527878761 \n",
      "\tlearning-rate:\t 0.5939305\n",
      "Iteration-Step: 8000\n",
      "\tAverage loss:\t 46.595563863277434 \n",
      "\tlearning-rate:\t 0.5043961\n",
      "Iteration-Step: 10000\n",
      "\tAverage loss:\t 36.85905154573918 \n",
      "\tlearning-rate:\t 0.4310915\n",
      "Iteration-Step: 12000\n",
      "\tAverage loss:\t 30.742217785716058 \n",
      "\tlearning-rate:\t 0.37107483\n",
      "Iteration-Step: 14000\n",
      "\tAverage loss:\t 26.198525524675848 \n",
      "\tlearning-rate:\t 0.32193726\n",
      "Iteration-Step: 16000\n",
      "\tAverage loss:\t 22.621717889904975 \n",
      "\tlearning-rate:\t 0.2817069\n",
      "Iteration-Step: 18000\n",
      "\tAverage loss:\t 19.930532382428645 \n",
      "\tlearning-rate:\t 0.24876902\n",
      "Iteration-Step: 20000\n",
      "\tAverage loss:\t 18.127147462308407 \n",
      "\tlearning-rate:\t 0.22180176\n",
      "Iteration-Step: 22000\n",
      "\tAverage loss:\t 16.32953436011076 \n",
      "\tlearning-rate:\t 0.19972284\n",
      "Iteration-Step: 24000\n",
      "\tAverage loss:\t 15.45440372055769 \n",
      "\tlearning-rate:\t 0.18164617\n",
      "Iteration-Step: 26000\n",
      "\tAverage loss:\t 14.765723244428635 \n",
      "\tlearning-rate:\t 0.16684623\n",
      "Iteration-Step: 28000\n",
      "\tAverage loss:\t 13.383648590683936 \n",
      "\tlearning-rate:\t 0.15472907\n",
      "Iteration-Step: 30000\n",
      "\tAverage loss:\t 12.788939649105073 \n",
      "\tlearning-rate:\t 0.14480837\n",
      "Iteration-Step: 32000\n",
      "\tAverage loss:\t 12.17216517674923 \n",
      "\tlearning-rate:\t 0.136686\n",
      "Iteration-Step: 34000\n",
      "\tAverage loss:\t 11.493418066561222 \n",
      "\tlearning-rate:\t 0.13003595\n",
      "Iteration-Step: 36000\n",
      "\tAverage loss:\t 11.468845460236073 \n",
      "\tlearning-rate:\t 0.12459136\n",
      "Iteration-Step: 38000\n",
      "\tAverage loss:\t 10.772964808315038 \n",
      "\tlearning-rate:\t 0.1201337\n",
      "Iteration-Step: 40000\n",
      "\tAverage loss:\t 10.475860629081726 \n",
      "\tlearning-rate:\t 0.116484076\n",
      "Iteration-Step: 42000\n",
      "\tAverage loss:\t 10.226835934400558 \n",
      "\tlearning-rate:\t 0.11349602\n",
      "Iteration-Step: 44000\n",
      "\tAverage loss:\t 9.991639833271503 \n",
      "\tlearning-rate:\t 0.11104961\n",
      "Iteration-Step: 46000\n",
      "\tAverage loss:\t 9.44657374638319 \n",
      "\tlearning-rate:\t 0.10904665\n",
      "Iteration-Step: 48000\n",
      "\tAverage loss:\t 9.722217267036438 \n",
      "\tlearning-rate:\t 0.10740678\n",
      "Iteration-Step: 50000\n",
      "\tAverage loss:\t 9.321798859477044 \n",
      "\tlearning-rate:\t 0.106064156\n",
      "Iteration-Step: 52000\n",
      "\tAverage loss:\t 9.05067025667429 \n",
      "\tlearning-rate:\t 0.10496491\n",
      "Iteration-Step: 54000\n",
      "\tAverage loss:\t 8.8022964951694 \n",
      "\tlearning-rate:\t 0.10406493\n",
      "Iteration-Step: 56000\n",
      "\tAverage loss:\t 8.740145417749881 \n",
      "\tlearning-rate:\t 0.10332808\n",
      "Iteration-Step: 58000\n",
      "\tAverage loss:\t 8.389185507714748 \n",
      "\tlearning-rate:\t 0.102724805\n",
      "Iteration-Step: 60000\n",
      "\tAverage loss:\t 8.757430337399244 \n",
      "\tlearning-rate:\t 0.10223088\n",
      "Iteration-Step: 62000\n",
      "\tAverage loss:\t 8.423793782234192 \n",
      "\tlearning-rate:\t 0.10182649\n",
      "Iteration-Step: 64000\n",
      "\tAverage loss:\t 8.277144762992858 \n",
      "\tlearning-rate:\t 0.1014954\n",
      "Iteration-Step: 66000\n",
      "\tAverage loss:\t 8.176459274500608 \n",
      "\tlearning-rate:\t 0.10122433\n",
      "Iteration-Step: 68000\n",
      "\tAverage loss:\t 8.01887924298644 \n",
      "\tlearning-rate:\t 0.1010024\n",
      "Iteration-Step: 70000\n",
      "\tAverage loss:\t 7.81869495511055 \n",
      "\tlearning-rate:\t 0.1008207\n",
      "Iteration-Step: 72000\n",
      "\tAverage loss:\t 8.012082870602608 \n",
      "\tlearning-rate:\t 0.10067193\n",
      "Iteration-Step: 74000\n",
      "\tAverage loss:\t 7.962744841635227 \n",
      "\tlearning-rate:\t 0.10055013\n",
      "Iteration-Step: 76000\n",
      "\tAverage loss:\t 7.723526112675667 \n",
      "\tlearning-rate:\t 0.10045041\n",
      "Iteration-Step: 78000\n",
      "\tAverage loss:\t 7.60614095517993 \n",
      "\tlearning-rate:\t 0.10036876\n",
      "Iteration-Step: 80000\n",
      "\tAverage loss:\t 7.610695923924446 \n",
      "\tlearning-rate:\t 0.10030192\n",
      "Iteration-Step: 82000\n",
      "\tAverage loss:\t 7.292008269190788 \n",
      "\tlearning-rate:\t 0.10024719\n",
      "Iteration-Step: 84000\n",
      "\tAverage loss:\t 7.692655746281147 \n",
      "\tlearning-rate:\t 0.10020238\n",
      "Iteration-Step: 86000\n",
      "\tAverage loss:\t 7.417393958479166 \n",
      "\tlearning-rate:\t 0.100165695\n",
      "Iteration-Step: 88000\n",
      "\tAverage loss:\t 7.278994256556034 \n",
      "\tlearning-rate:\t 0.10013566\n",
      "Iteration-Step: 90000\n",
      "\tAverage loss:\t 7.570352461397648 \n",
      "\tlearning-rate:\t 0.10011107\n",
      "Iteration-Step: 92000\n",
      "\tAverage loss:\t 7.309584422707558 \n",
      "\tlearning-rate:\t 0.100090936\n",
      "Iteration-Step: 94000\n",
      "\tAverage loss:\t 6.9899620885849 \n",
      "\tlearning-rate:\t 0.100074455\n",
      "Iteration-Step: 96000\n",
      "\tAverage loss:\t 7.368492931455374 \n",
      "\tlearning-rate:\t 0.100060955\n",
      "Iteration-Step: 98000\n",
      "\tAverage loss:\t 7.150819244652986 \n",
      "\tlearning-rate:\t 0.100049905\n",
      "Iteration-Step: 100000\n",
      "\tAverage loss:\t 7.136246445715427 \n",
      "\tlearning-rate:\t 0.10004086\n",
      "Iteration-Step: 102000\n",
      "\tAverage loss:\t 7.011101301938296 \n",
      "\tlearning-rate:\t 0.100033455\n",
      "Iteration-Step: 104000\n",
      "\tAverage loss:\t 6.929655001223088 \n",
      "\tlearning-rate:\t 0.10002739\n",
      "Iteration-Step: 106000\n",
      "\tAverage loss:\t 6.7613557087183 \n",
      "\tlearning-rate:\t 0.10002243\n",
      "Iteration-Step: 108000\n",
      "\tAverage loss:\t 7.104629929363727 \n",
      "\tlearning-rate:\t 0.10001836\n",
      "Iteration-Step: 110000\n",
      "\tAverage loss:\t 6.911049213290214 \n",
      "\tlearning-rate:\t 0.10001503\n",
      "Iteration-Step: 112000\n",
      "\tAverage loss:\t 6.820573176383972 \n",
      "\tlearning-rate:\t 0.10001231\n",
      "Iteration-Step: 114000\n",
      "\tAverage loss:\t 7.056371596723795 \n",
      "\tlearning-rate:\t 0.100010075\n",
      "Iteration-Step: 116000\n",
      "\tAverage loss:\t 6.798651538491249 \n",
      "\tlearning-rate:\t 0.10000825\n",
      "Iteration-Step: 118000\n",
      "\tAverage loss:\t 6.506776898294687 \n",
      "\tlearning-rate:\t 0.10000676\n",
      "Iteration-Step: 120000\n",
      "\tAverage loss:\t 6.905071799635887 \n",
      "\tlearning-rate:\t 0.10000553\n",
      "Iteration-Step: 122000\n",
      "\tAverage loss:\t 6.7474704916775226 \n",
      "\tlearning-rate:\t 0.10000453\n",
      "Iteration-Step: 124000\n",
      "\tAverage loss:\t 6.697245935201645 \n",
      "\tlearning-rate:\t 0.10000371\n",
      "Iteration-Step: 126000\n",
      "\tAverage loss:\t 6.59309020447731 \n",
      "\tlearning-rate:\t 0.100003034\n",
      "Iteration-Step: 128000\n",
      "\tAverage loss:\t 6.553368094533682 \n",
      "\tlearning-rate:\t 0.10000248\n",
      "Iteration-Step: 130000\n",
      "\tAverage loss:\t 6.333789168775081 \n",
      "\tlearning-rate:\t 0.100002035\n",
      "Iteration-Step: 132000\n",
      "\tAverage loss:\t 6.732066095679999 \n",
      "\tlearning-rate:\t 0.10000167\n",
      "Iteration-Step: 134000\n",
      "\tAverage loss:\t 6.529914895802737 \n",
      "\tlearning-rate:\t 0.100001365\n",
      "Iteration-Step: 136000\n",
      "\tAverage loss:\t 6.560302304178476 \n",
      "\tlearning-rate:\t 0.10000112\n",
      "Iteration-Step: 138000\n",
      "\tAverage loss:\t 6.640782338917256 \n",
      "\tlearning-rate:\t 0.10000092\n",
      "Iteration-Step: 140000\n",
      "\tAverage loss:\t 6.474942084670067 \n",
      "\tlearning-rate:\t 0.10000075\n",
      "Iteration-Step: 142000\n",
      "\tAverage loss:\t 6.20380734717846 \n",
      "\tlearning-rate:\t 0.10000061\n",
      "Iteration-Step: 144000\n",
      "\tAverage loss:\t 6.581110153734684 \n",
      "\tlearning-rate:\t 0.1000005\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    # Initialisieren alle Variablen\n",
    "    init.run()\n",
    "  \n",
    "    average_loss = 0\n",
    "  \n",
    "    # Train:\n",
    "    for step in xrange(num_steps):\n",
    "        # generieren der Batches\n",
    "        batch_inputs, batch_labels = generate_batch(batch_size, targets_list, labels_list)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels, iteration: step}\n",
    "    \n",
    "        # We perform one update step by evaluating the optimizer op (including it\n",
    "        # in the list of returned values for session.run()\n",
    "        # Ausführen eines einzelnen Update Steps, durch Evaluierung des GradientDescent Optimizers\n",
    "        # Minimieren des Loss und Updaten der Gewichte\n",
    "        _, loss_val, learn_rate = session.run(\n",
    "            [optimizer, loss, lr],\n",
    "            feed_dict=feed_dict)\n",
    "    \n",
    "        average_loss += loss_val\n",
    "        \n",
    "        # print Learing Rate:\n",
    "        if step % print_every_x_step == 0: \n",
    "            if step > 0:\n",
    "                average_loss /= print_every_x_step\n",
    "            print('Iteration-Step:', step)\n",
    "            print('\\tAverage loss:\\t', average_loss, '\\n\\tlearning-rate:\\t', learn_rate)\n",
    "            average_loss = 0\n",
    "        \n",
    "    # Generierte Embeddings für weitere Schritte verfügbar machen:\n",
    "    #final_embeddings = normalized_embeddings.eval()\n",
    "    final_embeddings = embeddings.eval()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dokumenten-Vektoren generieren:\n",
    "Je Anfragetext wird ein Dokumenten-Vektor aus den Word-Embeddings der jeweiligen Wörter des Textes abgeleitet.\n",
    "Dabei werden alle Word-Embeddings eines Dokuments spaltenweise gemittelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_embedding(doc):\n",
    "    '''\n",
    "    :param doc: list of sentences with lemmatized words\n",
    "    :return: document vector\n",
    "    '''\n",
    "    word_vecs = np.array([\n",
    "        final_embeddings[word_2_index_dict[word]] for sent in doc for word in sent if word in word_2_index_dict\n",
    "    ])\n",
    "    return np.mean(word_vecs, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dokumenten-Vektoren für alle Anfragen generieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['doc_vector'] = data['preprocessed'].apply(lambda prepr_text: get_doc_embedding(prepr_text) if prepr_text else np.nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete Null-Rows, da aufgrund von Fehlern in Preprocessing einige wenige leere Preprocessed Listen entstanden sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "description     0\n",
       "preprocessed    0\n",
       "textrank        0\n",
       "title           0\n",
       "doc_vector      7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "description     0\n",
       "preprocessed    0\n",
       "textrank        0\n",
       "title           0\n",
       "doc_vector      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data mit Dokumenten-Vektoren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>preprocessed</th>\n",
       "      <th>textrank</th>\n",
       "      <th>title</th>\n",
       "      <th>doc_vector</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30992</th>\n",
       "      <td>Besteht die Möglichkeit des elektronischen Dat...</td>\n",
       "      <td>[[krankenhaus, berlin], [möglichkeit, datenzug...</td>\n",
       "      <td>[[charité, Charité, 2.5233146811], [berlin, Be...</td>\n",
       "      <td>Datenaustausch von und zwischen Krankenhäusern...</td>\n",
       "      <td>[0.36203066, 0.28036064, 0.11504209, 0.0078134...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49906</th>\n",
       "      <td>1. Wann haben die beiden letzten lebensmittelr...</td>\n",
       "      <td>[[kontrollbericht, cube, stuttgart], [betriebs...</td>\n",
       "      <td>[[cube, Cube, 1.2479166667000001], [kontrollbe...</td>\n",
       "      <td>Kontrollbericht zu Cube, Stuttgart</td>\n",
       "      <td>[0.5573703, 0.3792328, 0.058914516, -0.0968474...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47330</th>\n",
       "      <td>1. Wann haben die beiden letzten lebensmittelr...</td>\n",
       "      <td>[[kontrollbericht, peter, pane, lübeck], [betr...</td>\n",
       "      <td>[[peter, Peter, 1.476089144], [pane, Pane, 1.4...</td>\n",
       "      <td>Kontrollbericht zu Peter Pane, Lübeck</td>\n",
       "      <td>[0.39963177, 0.21837826, 0.098461136, 0.130606...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25228</th>\n",
       "      <td>Dokumente / Informationen, aus denen hervorgeh...</td>\n",
       "      <td>[[bezeichnung], [beitragssservice], [dokument]...</td>\n",
       "      <td>[[bezeichnung, Bezeichnung, 1.0], [information...</td>\n",
       "      <td>Bezeichnung \"Beitragssservice\"</td>\n",
       "      <td>[0.50831634, 0.16496253, -0.09059725, 0.108449...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2981</th>\n",
       "      <td>Studie \"Politisch Netzaktive und Politik in De...</td>\n",
       "      <td>[[studie], [politisch, netzaktive, politik, de...</td>\n",
       "      <td>[[politik, Politik, 1.1900458333], [deutschlan...</td>\n",
       "      <td>Studie \"Politisch Netzaktive und Politik in De...</td>\n",
       "      <td>[0.66315424, 0.2967776, 0.16314766, -0.2264783...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             description  \\\n",
       "id                                                         \n",
       "30992  Besteht die Möglichkeit des elektronischen Dat...   \n",
       "49906  1. Wann haben die beiden letzten lebensmittelr...   \n",
       "47330  1. Wann haben die beiden letzten lebensmittelr...   \n",
       "25228  Dokumente / Informationen, aus denen hervorgeh...   \n",
       "2981   Studie \"Politisch Netzaktive und Politik in De...   \n",
       "\n",
       "                                            preprocessed  \\\n",
       "id                                                         \n",
       "30992  [[krankenhaus, berlin], [möglichkeit, datenzug...   \n",
       "49906  [[kontrollbericht, cube, stuttgart], [betriebs...   \n",
       "47330  [[kontrollbericht, peter, pane, lübeck], [betr...   \n",
       "25228  [[bezeichnung], [beitragssservice], [dokument]...   \n",
       "2981   [[studie], [politisch, netzaktive, politik, de...   \n",
       "\n",
       "                                                textrank  \\\n",
       "id                                                         \n",
       "30992  [[charité, Charité, 2.5233146811], [berlin, Be...   \n",
       "49906  [[cube, Cube, 1.2479166667000001], [kontrollbe...   \n",
       "47330  [[peter, Peter, 1.476089144], [pane, Pane, 1.4...   \n",
       "25228  [[bezeichnung, Bezeichnung, 1.0], [information...   \n",
       "2981   [[politik, Politik, 1.1900458333], [deutschlan...   \n",
       "\n",
       "                                                   title  \\\n",
       "id                                                         \n",
       "30992  Datenaustausch von und zwischen Krankenhäusern...   \n",
       "49906                 Kontrollbericht zu Cube, Stuttgart   \n",
       "47330              Kontrollbericht zu Peter Pane, Lübeck   \n",
       "25228                     Bezeichnung \"Beitragssservice\"   \n",
       "2981   Studie \"Politisch Netzaktive und Politik in De...   \n",
       "\n",
       "                                              doc_vector  \n",
       "id                                                        \n",
       "30992  [0.36203066, 0.28036064, 0.11504209, 0.0078134...  \n",
       "49906  [0.5573703, 0.3792328, 0.058914516, -0.0968474...  \n",
       "47330  [0.39963177, 0.21837826, 0.098461136, 0.130606...  \n",
       "25228  [0.50831634, 0.16496253, -0.09059725, 0.108449...  \n",
       "2981   [0.66315424, 0.2967776, 0.16314766, -0.2264783...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity:\n",
    "Cosine Similarity ist eine gängige Möglichkeit, um die Ähnlichkeit zweier Vektoren zu ermitteln. Mithilfe des Abstandswinkels zwischen den beiden Vektoren wird ein Wert zwischen 0 und 1 erzeugt welcher das Ähnlichkeitsmaß ausdrückt. Einfache, 2-Dimensionale Vektoren können noch hierbei noch per Hand errechnet werden. Cosine-Similarity funktioniert jedoch mit einer beliebigen Anzahl von Vektor-Dimensionen und ist somit für den Vergleich unserer Dokumenten- oder Wort-Vektoren hervoragend geeignet.\n",
    "\n",
    "Formel:\n",
    "![image](Images/Cosine_Similarity_Formula.svg)\n",
    "\n",
    "\n",
    "TODO/QUELLE: https://en.wikipedia.org/wiki/Cosine_similarity\n",
    "\n",
    "[1]:http://www.quotedb.com/quotes/2112\n",
    "\n",
    "Beispiel:\n",
    "\n",
    "|Person/Eigenschaft|  EG1 \t| EG2 \t|  EG3 \t|  EG4 \t|\n",
    "|:-----------:\t|:----:\t|:---:\t|:----:\t|:----:\t|\n",
    "|  Konstantin \t| -0.2 \t| 0.3 \t|  0.8 \t| -0.1 \t|\n",
    "|  Sebastian  \t|  0.5 \t| 0.6 \t| -0.9 \t|  0.5 \t|\n",
    "| Prof. Herta \t|  0.9 \t| 0.7 \t|  0.3 \t|  0.4 \t|\n",
    "\n",
    "Ähnlichkeit von Konstantin und Sebastian: <br>\n",
    "Cosine_Similarity ( [-0.2, 0.3, 0.8, -0.1] , [0.5, 0.6, -0.9, 0.5]) = -0.6046\n",
    "\n",
    "Ähnlichkeit von Sebastian und Prof. Herta: <br>\n",
    "Cosine_Similarity ( [0.5, 0.6, -0.9, 0.5], [0.9, 0.7, 0.3, 0.4]) = 0.2092"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(v1, v2):\n",
    "    n1 = np.linalg.norm(v1)\n",
    "    n2 = np.linalg.norm(v2)\n",
    "    return np.dot(v1, v2) / n1 / n2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity for herausgabe and bekleidung\n",
      "\t 0.39915022\n"
     ]
    }
   ],
   "source": [
    "#negativbeispiel\n",
    "word1 = 'herausgabe'\n",
    "word2 = 'bekleidung'\n",
    "print('cosine similarity for', word1, 'and', word2)\n",
    "print('\\t', similarity(final_embeddings[word_2_index_dict[word1]], final_embeddings[word_2_index_dict[word2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity for bfr and iarc\n",
      "\t 0.7267033\n"
     ]
    }
   ],
   "source": [
    "#positivbeispiel\n",
    "word1 = 'bfr'\n",
    "word2 = 'iarc'\n",
    "print('cosine similarity for', word1, 'and', word2)\n",
    "print('\\t', similarity(final_embeddings[word_2_index_dict[word1]], final_embeddings[word_2_index_dict[word2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_similarity(df, id_1, id_2):\n",
    "    print('Doc #1:')\n",
    "    print('\\tTitel:', df.loc[id_1]['title'])\n",
    "    \n",
    "    print('\\nDoc #2:')\n",
    "    print('\\tTitel:', df.loc[id_2]['title'])\n",
    "    \n",
    "    print('\\nSimilarity [0-1]:', similarity(df.loc[id_1]['doc_vector'], df.loc[id_2]['doc_vector']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc #1:\n",
      "\tTitel: Kontrollbericht zu Bartz, Arzfeld\n",
      "\n",
      "Doc #2:\n",
      "\tTitel: Kontrollbericht zu Mamma Italia, Esslingen am Neckar\n",
      "\n",
      "Similarity [0-1]: 0.95707035\n"
     ]
    }
   ],
   "source": [
    "#positivbesipiel\n",
    "doc_similarity(data, 58945, 48729)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc #1:\n",
      "\tTitel: Kontrollbericht zu Bartz, Arzfeld\n",
      "\n",
      "Doc #2:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.virtualenvs/testpy3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2656\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine._get_loc_duplicates\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.index.Int64Engine._maybe_get_bool_indexer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-946b36b72155>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#negativbeispiel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdoc_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m58945\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-c5274daa821b>\u001b[0m in \u001b[0;36mdoc_similarity\u001b[0;34m(df, id_1, id_2)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nDoc #2:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\tTitel:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nSimilarity [0-1]:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid_1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'doc_vector'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'doc_vector'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/testpy3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/testpy3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1911\u001b[0m         \u001b[0;31m# fall thru to straight lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1912\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1913\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/testpy3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_label\u001b[0;34m(self, label, axis)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'no slices here, handle elsewhere'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/testpy3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mxs\u001b[0;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[1;32m   3583\u001b[0m                                                       drop_level=drop_level)\n\u001b[1;32m   3584\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3585\u001b[0;31m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3587\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/testpy3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2657\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine._get_loc_duplicates\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.index.Int64Engine._maybe_get_bool_indexer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "#negativbeispiel\n",
    "doc_similarity(data, 58945, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da TSNE recht rechenintensiv ist wird aus Performancegründen nur eine Stichprobe der Anfragen betrachtet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tsne_doc(data_df, amount, dimension=2, perplexity=30, learning_rate=200, n_iter=5000):\n",
    "    '''\n",
    "    computes tsne emebeddings for random set of documents\n",
    "    :param data_df: pandas datafram with at least doc_vector and title columns\n",
    "    :param amount: amount of documents to generate tsne for\n",
    "    :param dimension: 2 for 2D or 3 for 3D\n",
    "    '''\n",
    "    tsne = TSNE(perplexity=perplexity, learning_rate=learning_rate, n_components=dimension, init='pca', n_iter=n_iter, method='exact', verbose=1)\n",
    "    sample = data_df.sample(n=amount)\n",
    "\n",
    "    doc_vecs = np.array([doc_vec for doc_vec in sample['doc_vector'].values])\n",
    "    tsne_embeddings = tsne.fit_transform(doc_vecs)\n",
    "\n",
    "    labels = sample['title'].values\n",
    "    return tsne_embeddings, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quick n dirty 2d plot\n",
    "Labels überlappen sich\n",
    "\n",
    "Am besten später 3d plot mit plotly und Labels werden nur bei hover angezeigt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_tsne(embeddings, labels):\n",
    "    plt.figure(figsize=(18,18))\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = embeddings[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(\n",
    "            label,\n",
    "            xy=(x, y),\n",
    "            xytext=(5, 2),\n",
    "            textcoords='offset points',\n",
    "            va='bottom'\n",
    "        )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaktiver 2D/3D Plot mit ploty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotly_plot_tsne(tsne_embeddings, labels, dimension=3):\n",
    "    \n",
    "    marker=dict(\n",
    "                size=6,\n",
    "                line=dict(\n",
    "                    color='rgb(225, 225, 225)',\n",
    "                    width=0.5\n",
    "                ),\n",
    "                opacity=1\n",
    "            )\n",
    "    \n",
    "    if dimension==3:\n",
    "        x, y, z = zip(*tsne_embeddings)\n",
    "        \n",
    "        trace1 = go.Scatter3d(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            z=z,\n",
    "            mode='markers',\n",
    "            marker=marker,\n",
    "            text=labels,\n",
    "            hoverinfo='text'\n",
    "        )\n",
    "    else:\n",
    "        x, y = zip(*tsne_embeddings)\n",
    "        \n",
    "        trace1 = go.Scatter(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            mode='markers',\n",
    "            marker=marker,\n",
    "            text=labels,\n",
    "            hoverinfo='text'\n",
    "        )\n",
    "    \n",
    "    \n",
    "\n",
    "    data = [trace1]\n",
    "    layout = go.Layout(\n",
    "        margin=dict(\n",
    "            l=0,\n",
    "            r=0,\n",
    "            b=0,\n",
    "            t=0\n",
    "        ),\n",
    "        xaxis = dict(\n",
    "            zeroline = False\n",
    "        ),\n",
    "        yaxis = dict(\n",
    "            zeroline = False\n",
    "        ),\n",
    "        width=1000,\n",
    "        height=800,\n",
    "        #paper_bgcolor= 'rgb(240, 240, 240)',\n",
    "        #plot_bgcolor= 'rgb(240, 240, 240)'\n",
    "    )\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D TSNE Visualisierung für Stichprobe von Anfragen\n",
    "Auch wenn es aufgrund der überlappenden Labels noch recht schwer zu erkennen ist, kann man sehen, dass sich Anfragen ähnlicher Thematik ballen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_x_docs = 200\n",
    "tsne_embeddings_2d, labels_2d = compute_tsne_doc(\n",
    "    data, \n",
    "    plot_x_docs, \n",
    "    dimension=2,\n",
    "    perplexity=25,\n",
    "    learning_rate=10,\n",
    "    n_iter=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_2d_tsne(tsne_embeddings_2d, labels_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotly_plot_tsne(tsne_embeddings_2d, labels_2d, dimension=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D TSNE Visualisierung für Stichprobe von Anfragen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_x_docs = 200\n",
    "tsne_embeddings_3d, labels_3d = compute_tsne_doc(\n",
    "    data, \n",
    "    plot_x_docs, \n",
    "    dimension=3,\n",
    "    perplexity=25,\n",
    "    learning_rate=10,\n",
    "    n_iter=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotly_plot_tsne(tsne_embeddings_3d, labels_3d, dimension=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FDS_Word2Vec.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python3.6.7 (testpy3)",
   "language": "python",
   "name": "testpy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
